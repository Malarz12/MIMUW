{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a85cf26c-ae6d-4829-af52-4635b6db6a2a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pip in ./.venv/lib/python3.9/site-packages (25.1.1)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: numpy in ./.venv/lib/python3.9/site-packages (2.0.2)\n",
      "Requirement already satisfied: pandas in ./.venv/lib/python3.9/site-packages (2.2.3)\n",
      "Requirement already satisfied: scipy in ./.venv/lib/python3.9/site-packages (1.13.1)\n",
      "Requirement already satisfied: statsmodels in ./.venv/lib/python3.9/site-packages (0.14.4)\n",
      "Requirement already satisfied: matplotlib in ./.venv/lib/python3.9/site-packages (3.9.4)\n",
      "Requirement already satisfied: seaborn in ./.venv/lib/python3.9/site-packages (0.13.2)\n",
      "Requirement already satisfied: scikit-learn in ./.venv/lib/python3.9/site-packages (1.6.1)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in ./.venv/lib/python3.9/site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in ./.venv/lib/python3.9/site-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in ./.venv/lib/python3.9/site-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: patsy>=0.5.6 in ./.venv/lib/python3.9/site-packages (from statsmodels) (1.0.1)\n",
      "Requirement already satisfied: packaging>=21.3 in ./.venv/lib/python3.9/site-packages (from statsmodels) (25.0)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in ./.venv/lib/python3.9/site-packages (from matplotlib) (1.3.0)\n",
      "Requirement already satisfied: cycler>=0.10 in ./.venv/lib/python3.9/site-packages (from matplotlib) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in ./.venv/lib/python3.9/site-packages (from matplotlib) (4.58.0)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in ./.venv/lib/python3.9/site-packages (from matplotlib) (1.4.7)\n",
      "Requirement already satisfied: pillow>=8 in ./.venv/lib/python3.9/site-packages (from matplotlib) (11.2.1)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in ./.venv/lib/python3.9/site-packages (from matplotlib) (3.2.3)\n",
      "Requirement already satisfied: importlib-resources>=3.2.0 in ./.venv/lib/python3.9/site-packages (from matplotlib) (6.5.2)\n",
      "Requirement already satisfied: joblib>=1.2.0 in ./.venv/lib/python3.9/site-packages (from scikit-learn) (1.5.1)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in ./.venv/lib/python3.9/site-packages (from scikit-learn) (3.6.0)\n",
      "Requirement already satisfied: zipp>=3.1.0 in ./.venv/lib/python3.9/site-packages (from importlib-resources>=3.2.0->matplotlib) (3.21.0)\n",
      "Requirement already satisfied: six>=1.5 in ./.venv/lib/python3.9/site-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: gdown in ./.venv/lib/python3.9/site-packages (5.2.0)\n",
      "Requirement already satisfied: torch in ./.venv/lib/python3.9/site-packages (2.7.0)\n",
      "Requirement already satisfied: beautifulsoup4 in ./.venv/lib/python3.9/site-packages (from gdown) (4.13.4)\n",
      "Requirement already satisfied: filelock in ./.venv/lib/python3.9/site-packages (from gdown) (3.18.0)\n",
      "Requirement already satisfied: requests[socks] in ./.venv/lib/python3.9/site-packages (from gdown) (2.32.3)\n",
      "Requirement already satisfied: tqdm in ./.venv/lib/python3.9/site-packages (from gdown) (4.67.1)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in ./.venv/lib/python3.9/site-packages (from torch) (4.13.2)\n",
      "Requirement already satisfied: sympy>=1.13.3 in ./.venv/lib/python3.9/site-packages (from torch) (1.14.0)\n",
      "Requirement already satisfied: networkx in ./.venv/lib/python3.9/site-packages (from torch) (3.2.1)\n",
      "Requirement already satisfied: jinja2 in ./.venv/lib/python3.9/site-packages (from torch) (3.1.6)\n",
      "Requirement already satisfied: fsspec in ./.venv/lib/python3.9/site-packages (from torch) (2025.5.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in ./.venv/lib/python3.9/site-packages (from sympy>=1.13.3->torch) (1.3.0)\n",
      "Requirement already satisfied: soupsieve>1.2 in ./.venv/lib/python3.9/site-packages (from beautifulsoup4->gdown) (2.7)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in ./.venv/lib/python3.9/site-packages (from jinja2->torch) (3.0.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in ./.venv/lib/python3.9/site-packages (from requests[socks]->gdown) (3.4.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in ./.venv/lib/python3.9/site-packages (from requests[socks]->gdown) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in ./.venv/lib/python3.9/site-packages (from requests[socks]->gdown) (2.4.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in ./.venv/lib/python3.9/site-packages (from requests[socks]->gdown) (2025.4.26)\n",
      "Requirement already satisfied: PySocks!=1.5.7,>=1.5.6 in ./.venv/lib/python3.9/site-packages (from requests[socks]->gdown) (1.7.1)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install --upgrade pip\n",
    "%pip install numpy pandas scipy statsmodels matplotlib seaborn scikit-learn\n",
    "%pip install gdown torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "ccc3f9aa-ccf5-4914-b894-e107cd2baaa8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading...\n",
      "From: https://drive.google.com/uc?export=download&id=1vJ18xuP32a6hDdwSUePXRNllPKRPZqN5\n",
      "To: /Users/michalsmilowski/pythonProject14/zad2_wum_data_for_students.csv\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 14.6M/14.6M [00:04<00:00, 3.51MB/s]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Class</th>\n",
       "      <th>Output</th>\n",
       "      <th>Input1</th>\n",
       "      <th>Input2</th>\n",
       "      <th>Input3</th>\n",
       "      <th>Input4</th>\n",
       "      <th>Input5</th>\n",
       "      <th>Input6</th>\n",
       "      <th>Input7</th>\n",
       "      <th>Input8</th>\n",
       "      <th>...</th>\n",
       "      <th>Input391</th>\n",
       "      <th>Input392</th>\n",
       "      <th>Input393</th>\n",
       "      <th>Input394</th>\n",
       "      <th>Input395</th>\n",
       "      <th>Input396</th>\n",
       "      <th>Input397</th>\n",
       "      <th>Input398</th>\n",
       "      <th>Input399</th>\n",
       "      <th>Input400</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0.800586</td>\n",
       "      <td>-0.002583</td>\n",
       "      <td>2.184037</td>\n",
       "      <td>-0.322008</td>\n",
       "      <td>1.621241</td>\n",
       "      <td>1.192444</td>\n",
       "      <td>-0.278356</td>\n",
       "      <td>-0.207366</td>\n",
       "      <td>0.735689</td>\n",
       "      <td>...</td>\n",
       "      <td>-2.140861</td>\n",
       "      <td>1.187660</td>\n",
       "      <td>0.345238</td>\n",
       "      <td>-0.844885</td>\n",
       "      <td>0.580007</td>\n",
       "      <td>-2.605781</td>\n",
       "      <td>-0.299471</td>\n",
       "      <td>0.711487</td>\n",
       "      <td>-0.753316</td>\n",
       "      <td>0.728763</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>2.168475</td>\n",
       "      <td>0.668637</td>\n",
       "      <td>1.373933</td>\n",
       "      <td>-0.476868</td>\n",
       "      <td>-0.724704</td>\n",
       "      <td>0.031162</td>\n",
       "      <td>-1.845921</td>\n",
       "      <td>0.784890</td>\n",
       "      <td>1.508526</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.286120</td>\n",
       "      <td>-0.900044</td>\n",
       "      <td>-0.500399</td>\n",
       "      <td>-0.126421</td>\n",
       "      <td>-0.632233</td>\n",
       "      <td>-2.557419</td>\n",
       "      <td>0.056044</td>\n",
       "      <td>0.634774</td>\n",
       "      <td>-0.259835</td>\n",
       "      <td>0.106390</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>-1.210777</td>\n",
       "      <td>-0.681438</td>\n",
       "      <td>-0.544753</td>\n",
       "      <td>0.441346</td>\n",
       "      <td>-0.019906</td>\n",
       "      <td>-0.192135</td>\n",
       "      <td>-0.162510</td>\n",
       "      <td>-0.998777</td>\n",
       "      <td>0.686472</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.391605</td>\n",
       "      <td>-0.190147</td>\n",
       "      <td>0.793746</td>\n",
       "      <td>-0.812737</td>\n",
       "      <td>-0.068228</td>\n",
       "      <td>-0.313143</td>\n",
       "      <td>2.564096</td>\n",
       "      <td>0.848355</td>\n",
       "      <td>0.180556</td>\n",
       "      <td>-1.525615</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>0.505678</td>\n",
       "      <td>-0.497957</td>\n",
       "      <td>0.720712</td>\n",
       "      <td>0.149120</td>\n",
       "      <td>0.019251</td>\n",
       "      <td>1.377850</td>\n",
       "      <td>0.981337</td>\n",
       "      <td>-0.846813</td>\n",
       "      <td>0.036790</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.176734</td>\n",
       "      <td>-0.947351</td>\n",
       "      <td>-0.888601</td>\n",
       "      <td>1.509450</td>\n",
       "      <td>-0.501929</td>\n",
       "      <td>-0.554909</td>\n",
       "      <td>-0.104051</td>\n",
       "      <td>0.442150</td>\n",
       "      <td>-0.056644</td>\n",
       "      <td>1.447267</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>-10.281033</td>\n",
       "      <td>-1.178544</td>\n",
       "      <td>0.176941</td>\n",
       "      <td>1.112202</td>\n",
       "      <td>1.234189</td>\n",
       "      <td>0.999451</td>\n",
       "      <td>-0.773329</td>\n",
       "      <td>-0.811075</td>\n",
       "      <td>1.550537</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.181325</td>\n",
       "      <td>0.198960</td>\n",
       "      <td>-0.697497</td>\n",
       "      <td>-0.836371</td>\n",
       "      <td>1.652071</td>\n",
       "      <td>0.974292</td>\n",
       "      <td>1.584071</td>\n",
       "      <td>-0.202352</td>\n",
       "      <td>1.362426</td>\n",
       "      <td>1.023857</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 402 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   Class     Output    Input1    Input2    Input3    Input4    Input5  \\\n",
       "0      0   0.800586 -0.002583  2.184037 -0.322008  1.621241  1.192444   \n",
       "1      0   2.168475  0.668637  1.373933 -0.476868 -0.724704  0.031162   \n",
       "2      1  -1.210777 -0.681438 -0.544753  0.441346 -0.019906 -0.192135   \n",
       "3      1   0.505678 -0.497957  0.720712  0.149120  0.019251  1.377850   \n",
       "4      1 -10.281033 -1.178544  0.176941  1.112202  1.234189  0.999451   \n",
       "\n",
       "     Input6    Input7    Input8  ...  Input391  Input392  Input393  Input394  \\\n",
       "0 -0.278356 -0.207366  0.735689  ... -2.140861  1.187660  0.345238 -0.844885   \n",
       "1 -1.845921  0.784890  1.508526  ... -1.286120 -0.900044 -0.500399 -0.126421   \n",
       "2 -0.162510 -0.998777  0.686472  ... -0.391605 -0.190147  0.793746 -0.812737   \n",
       "3  0.981337 -0.846813  0.036790  ... -0.176734 -0.947351 -0.888601  1.509450   \n",
       "4 -0.773329 -0.811075  1.550537  ... -0.181325  0.198960 -0.697497 -0.836371   \n",
       "\n",
       "   Input395  Input396  Input397  Input398  Input399  Input400  \n",
       "0  0.580007 -2.605781 -0.299471  0.711487 -0.753316  0.728763  \n",
       "1 -0.632233 -2.557419  0.056044  0.634774 -0.259835  0.106390  \n",
       "2 -0.068228 -0.313143  2.564096  0.848355  0.180556 -1.525615  \n",
       "3 -0.501929 -0.554909 -0.104051  0.442150 -0.056644  1.447267  \n",
       "4  1.652071  0.974292  1.584071 -0.202352  1.362426  1.023857  \n",
       "\n",
       "[5 rows x 402 columns]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 1) Jeśli jeszcze nie masz zainstalowanego gdown, uruchom poniższą komórkę:\n",
    "!pip install gdown --quiet\n",
    "\n",
    "# 2) Importujemy gdown i pobieramy plik:\n",
    "import gdown\n",
    "\n",
    "# ID pliku wyciągnięte z linku \"https://drive.google.com/file/d/1vJ18xuP32a6hDdwSUePXRNllPKRPZqN5/view\"\n",
    "file_id = \"1vJ18xuP32a6hDdwSUePXRNllPKRPZqN5\"\n",
    "url = f\"https://drive.google.com/uc?export=download&id={file_id}\"\n",
    "\n",
    "# Ustawiamy nazwę, pod którą plik zostanie zapisany lokalnie\n",
    "output_name = \"zad2_wum_data_for_students.csv\"\n",
    "\n",
    "# Pobieramy plik do bieżącego katalogu\n",
    "gdown.download(url, output_name, quiet=False)\n",
    "\n",
    "# 3) Wczytujemy pobrany plik do pandas:\n",
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv(output_name, sep=\";\")\n",
    "df.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "277a3c58-edc1-4b92-9cc2-26550a8186df",
   "metadata": {},
   "source": [
    "**Baseline Models**\n",
    "\n",
    "We start by defining simple baseline models to establish a “floor” performance that any more sophisticated method should exceed:\n",
    "\n",
    "- **Linear Regression** for the continuous target (`Output`)  \n",
    "- **Logistic Regression** for the categorical target (`Class`)\n",
    "\n",
    "Both models are evaluated in two ways:  \n",
    "1. **Train/Test Split** (80% training, 20% testing)  \n",
    "2. **10-Fold Cross-Validation** (to gauge stability and generalization)\n",
    "\n",
    "---\n",
    "\n",
    "**Results**\n",
    "\n",
    "- The **linear regression** model achieves an \\(R^2\\) of approximately **0.37**, indicating it explains only 37% of the variance in the continuous target—far from ideal.  \n",
    "- The **logistic regression** model attains an **accuracy** of around **0.50**, which is barely better than random guessing in a binary or balanced multiclass scenario.\n",
    "\n",
    "These poor scores confirm that simple linear methods are insufficient here and motivate the use of more advanced techniques (regularization, feature engineering, non-linear models, etc.).  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "e3851a3f-275b-421b-a9cd-1d5380a59fa5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Linear Regression R² on TEST data: 0.4463990746888591\n",
      "Linear Regression R² on CV folds: [0.55195921 0.35236736 0.23694276 0.32884444 0.46170815 0.38280338\n",
      " 0.15756861 0.43151613 0.39775886 0.45489592]\n",
      "Mean CV R²: 0.37563648074364064\n",
      "\n",
      "Logistic Regression accuracy on TEST data: 0.5175\n",
      "Logistic Regression accuracy on CV folds: [0.495 0.475 0.485 0.505 0.545 0.555 0.525 0.52  0.51  0.485]\n",
      "Mean CV accuracy: 0.51\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split, cross_val_score, KFold\n",
    "from sklearn.linear_model   import LinearRegression, LogisticRegression\n",
    "from sklearn.metrics        import accuracy_score\n",
    "\n",
    "# prepare data\n",
    "X   = df.drop(['Output', 'Class'], axis=1)\n",
    "y_r = df['Output']\n",
    "y_c = df['Class']\n",
    "\n",
    "# 1) Linear regression\n",
    "X_r_train, X_r_test, y_r_train, y_r_test = train_test_split(\n",
    "    X, y_r, test_size=0.2, random_state=42\n",
    ")\n",
    "model_reg = LinearRegression().fit(X_r_train, y_r_train)\n",
    "r2 = model_reg.score(X_r_test, y_r_test)\n",
    "print(\"Linear Regression R² on TEST data:\", r2)\n",
    "\n",
    "# Cross-validation\n",
    "kf     = KFold(n_splits=10, shuffle=True, random_state=42)\n",
    "r2_cv  = cross_val_score(LinearRegression(), X, y_r,\n",
    "                         cv=kf, scoring='r2')\n",
    "print(\"Linear Regression R² on CV folds:\", r2_cv)\n",
    "print(\"Mean CV R²:\", r2_cv.mean())\n",
    "\n",
    "print()\n",
    "\n",
    "# 2) Logistic regression\n",
    "X_c_train, X_c_test, y_c_train, y_c_test = train_test_split(\n",
    "    X, y_c, test_size=0.2, random_state=42\n",
    ")\n",
    "model_clf = LogisticRegression(max_iter=1000, random_state=42)\n",
    "model_clf.fit(X_c_train, y_c_train)\n",
    "y_c_pred = model_clf.predict(X_c_test)\n",
    "acc = accuracy_score(y_c_test, y_c_pred)\n",
    "print(\"Logistic Regression accuracy on TEST data:\", acc)\n",
    "\n",
    "# Cross-validation\n",
    "acc_cv = cross_val_score(LogisticRegression(max_iter=1000, random_state=42),\n",
    "                         X, y_c, cv=kf, scoring='accuracy')\n",
    "print(\"Logistic Regression accuracy on CV folds:\", acc_cv)\n",
    "print(\"Mean CV accuracy:\", acc_cv.mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ace11e11-9487-4346-96bf-9d762e4a6ddf",
   "metadata": {},
   "source": [
    "### Detailed Model Descriptions\n",
    "\n",
    "Below is a concise explanation of how each classifier operates and why it behaves the way it does on this dataset.\n",
    "\n",
    "---\n",
    "\n",
    "#### SVC Pipeline\n",
    "This model first uses **SelectKBest** (univariate statistical test) to pick the 10 most discriminative features, then trains a **Support Vector Classifier** with an RBF (radial-basis) kernel.  \n",
    "- **Selected features** (k = 10):  \n",
    "  - Input2  \n",
    "  - Input40  \n",
    "  - Input41  \n",
    "  - Input74  \n",
    "  - Input238  \n",
    "  - Input240  \n",
    "  - Input293  \n",
    "  - Input308  \n",
    "  - Input330  \n",
    "  - Input396  \n",
    "- **Feature selection** reduces noise and focuses the SVM on the strongest signals.  \n",
    "- The **RBF kernel** maps data into a high-dimensional space where classes can be separated by a hyperplane, handling nonlinearity.  \n",
    "- Overall, this two-step approach balances dimensionality reduction with a powerful nonlinear classifier, yielding the best accuracy.\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "#### k-Nearest Neighbors (k-NN)\n",
    "k-NN is a **lazy**, instance-based learner that predicts a new point’s class by majority vote among its k closest neighbors in feature space.  \n",
    "- **Manhattan distance** measures absolute coordinate differences, which can be more robust when features vary on different scales.  \n",
    "- Because k-NN makes no global model assumptions, its performance directly reflects the local structure of the data; here, moderate accuracy suggests some local clusters but also considerable overlap between classes.\n",
    "\n",
    "---\n",
    "\n",
    "#### SVM (RBF, C=1)\n",
    "A standalone **RBF SVM** trains on all features without explicit feature selection.  \n",
    "- The RBF kernel allows the model to carve out complex, curved decision boundaries.  \n",
    "- The regularization parameter C balances margin width against misclassification: a lower C yields smoother (more general) boundaries.  \n",
    "- Its relatively poor performance indicates that without prior feature filtering, the SVM overfits or struggles to ignore irrelevant dimensions.\n",
    "\n",
    "---\n",
    "\n",
    "#### Random Forest\n",
    "An ensemble of decision trees, each trained on a bootstrap sample of the data and a random subset of features at each split.  \n",
    "- **Bagging** (bootstrap aggregating) reduces variance by averaging many trees.  \n",
    "- Random feature subsetting decorrelates individual trees, further stabilizing predictions.  \n",
    "- Decision trees capture nonlinear interactions and feature thresholds, so the forest excels where linear methods fail.\n",
    "\n",
    "---\n",
    "\n",
    "#### Linear SVC\n",
    "A linear-kernel SVM uses the same principle as logistic regression—finding a hyperplane that maximizes the margin between classes—but optimizes a hinge loss instead of likelihood.  \n",
    "- It can be more robust to outliers than logistic regression.  \n",
    "- However, like any linear method, it struggles when class structure is highly nonlinear.\n",
    "\n",
    "---\n",
    "\n",
    "#### Extra Trees\n",
    "“Extremely randomized” trees follow the same ensemble principle as Random Forests but choose split thresholds at random rather than optimally.  \n",
    "- This extra randomness often reduces variance and can speed up training.  \n",
    "- The results suggest that random splits still capture enough structure to outperform pure linear models, though they trail the more systematically constructed Random Forest.\n",
    "\n",
    "---\n",
    "\n",
    "#### Gaussian Naïve Bayes\n",
    "Assumes each feature is normally distributed and **conditionally independent** given the class.  \n",
    "- Computes class probabilities via Bayes’ theorem under a diagonal covariance assumption.  \n",
    "- Extremely fast to train, but the independence assumption is strong; correlated features or non-Gaussian distributions degrade performance.  \n",
    "\n",
    "---\n",
    "\n",
    "**Why the differences?**  \n",
    "- **Linear models** (Logistic, Linear SVC) and **Naïve Bayes** rely on simple assumptions and often underfit complex data.  \n",
    "- **k-NN** leverages local neighborhoods but suffers when classes overlap heavily.  \n",
    "- **Kernel methods** (RBF SVM) and **trees** (Random Forest, Extra Trees) can learn nonlinear relationships, so they perform better when the decision boundary is not a straight line.  \n",
    "- Combining **feature selection** with a flexible classifier (SVC Pipeline) often yields the best balance of generalization and expressivity.  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "ee838f66-b648-41b0-b66c-d41586bfa989",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== 10-fold CV – average metrics for each model ===\n",
      "                              Accuracy  Precision    Recall  F1-score\n",
      "Model                                                                \n",
      "SVC Pipeline                    0.8155   0.816163  0.815289  0.815305\n",
      "k-NN (Manhattan, k=50)          0.6130   0.615445  0.612244  0.610479\n",
      "SVM (RBF, C=1)                  0.5585   0.558515  0.558111  0.557468\n",
      "RF (800 trees, max_depth=30)    0.6875   0.690017  0.686666  0.685825\n",
      "Linear SVC                      0.5285   0.528459  0.528432  0.528134\n",
      "Extra Trees                     0.6595   0.659998  0.659233  0.659044\n",
      "Gaussian NB                     0.5870   0.587172  0.586922  0.586627\n",
      "\n",
      "Model: SVC Pipeline\n",
      "Wybrane cechy (SelectKBest, k=10):\n",
      "  - Input2\n",
      "  - Input40\n",
      "  - Input41\n",
      "  - Input74\n",
      "  - Input238\n",
      "  - Input240\n",
      "  - Input293\n",
      "  - Input308\n",
      "  - Input330\n",
      "  - Input396\n",
      "  → Accuracy:  0.8075\n",
      "    Precision: 0.8076\n",
      "    Recall:    0.8076\n",
      "    F1-score:  0.8075\n",
      "  → Accuracy:  0.6175\n",
      "    Precision: 0.6175\n",
      "    Recall:    0.6171\n",
      "    F1-score:  0.6170\n",
      "  → Accuracy:  0.5525\n",
      "    Precision: 0.5526\n",
      "    Recall:    0.5526\n",
      "    F1-score:  0.5525\n",
      "  → Accuracy:  0.7100\n",
      "    Precision: 0.7115\n",
      "    Recall:    0.7093\n",
      "    F1-score:  0.7090\n",
      "  → Accuracy:  0.5475\n",
      "    Precision: 0.5486\n",
      "    Recall:    0.5482\n",
      "    F1-score:  0.5469\n",
      "  → Accuracy:  0.6375\n",
      "    Precision: 0.6379\n",
      "    Recall:    0.6369\n",
      "    F1-score:  0.6365\n",
      "  → Accuracy:  0.5675\n",
      "    Precision: 0.5680\n",
      "    Recall:    0.5679\n",
      "    F1-score:  0.5674\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.feature_selection import SelectKBest, f_classif\n",
    "from sklearn.svm import SVC, LinearSVC\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.ensemble import (\n",
    "    RandomForestClassifier,\n",
    "    ExtraTreesClassifier,\n",
    "    HistGradientBoostingClassifier\n",
    ")\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import StratifiedKFold, cross_validate, train_test_split\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score,\n",
    "    precision_score,\n",
    "    recall_score,\n",
    "    f1_score\n",
    ")\n",
    "\n",
    "# 0) Load data\n",
    "df = pd.read_csv(file_name, sep=\";\")\n",
    "input_cols = [c for c in df.columns if c.lower().startswith(\"input\")]\n",
    "X = df[input_cols]\n",
    "y = df['Class']\n",
    "\n",
    "# 1) Define models\n",
    "models = {\n",
    "    \"SVC Pipeline\": Pipeline([\n",
    "        (\"select\", SelectKBest(score_func=f_classif, k=10)),\n",
    "        (\"svc\",    SVC(kernel=\"rbf\", C=100.0, gamma=\"scale\", random_state=42))\n",
    "    ]),\n",
    "    \"k-NN (Manhattan, k=50)\": KNeighborsClassifier(\n",
    "        n_neighbors=50, metric=\"manhattan\"\n",
    "    ),\n",
    "    \"SVM (RBF, C=1)\": SVC(kernel=\"rbf\", C=1.0, gamma=\"scale\", random_state=42),\n",
    "    \"RF (800 trees, max_depth=30)\": RandomForestClassifier(\n",
    "        n_estimators=800, max_depth=30, random_state=42\n",
    "    ),\n",
    "    \"Linear SVC\": Pipeline([\n",
    "        (\"scaler\", StandardScaler()),\n",
    "        (\"lsvc\",   LinearSVC(max_iter=5000, random_state=42))\n",
    "    ]),\n",
    "    \"Extra Trees\": ExtraTreesClassifier(\n",
    "        n_estimators=200, max_depth=None, random_state=42\n",
    "    ),\n",
    "    \"Gaussian NB\": Pipeline([\n",
    "        (\"scaler\", StandardScaler()),\n",
    "        (\"gnb\",    GaussianNB())\n",
    "    ])\n",
    "}\n",
    "\n",
    "# 2) 10-fold Stratified CV – only average metrics\n",
    "cv = StratifiedKFold(n_splits=10, shuffle=True, random_state=42)\n",
    "scoring = {\n",
    "    'accuracy':  'accuracy',\n",
    "    'precision': 'precision_macro',\n",
    "    'recall':    'recall_macro',\n",
    "    'f1':        'f1_macro'\n",
    "}\n",
    "\n",
    "results_cv = []\n",
    "for name, model in models.items():\n",
    "    cv_res = cross_validate(\n",
    "        model, X, y, cv=cv, scoring=scoring, n_jobs=-1, return_train_score=False\n",
    "    )\n",
    "    results_cv.append({\n",
    "        \"Model\":     name,\n",
    "        \"Accuracy\":  np.mean(cv_res['test_accuracy']),\n",
    "        \"Precision\": np.mean(cv_res['test_precision']),\n",
    "        \"Recall\":    np.mean(cv_res['test_recall']),\n",
    "        \"F1-score\":  np.mean(cv_res['test_f1'])\n",
    "    })\n",
    "\n",
    "metrics_cv_df = pd.DataFrame(results_cv).set_index(\"Model\")\n",
    "print(\"=== 10-fold CV – average metrics for each model ===\")\n",
    "print(metrics_cv_df)\n",
    "\n",
    "# 3) 80/20 train-test split evaluation\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=41, stratify=y\n",
    ")\n",
    "\n",
    "for name, model in models.items():\n",
    "    # Dopasuj model na zbiorze treningowym\n",
    "    model.fit(X_train, y_train)\n",
    "    \n",
    "    if hasattr(model, \"named_steps\") and \"select\" in model.named_steps:\n",
    "        selector = model.named_steps[\"select\"]\n",
    "        mask = selector.get_support()\n",
    "        selected_feats = [feat for feat, use in zip(input_cols, mask) if use]\n",
    "        \n",
    "        print(f\"\\nModel: {name}\")\n",
    "        print(\"Wybrane cechy (SelectKBest, k=10):\")\n",
    "        for feat in selected_feats:\n",
    "            print(\"  -\", feat)\n",
    "    \n",
    "    y_pred = model.predict(X_test)\n",
    "    acc  = accuracy_score(y_test, y_pred)\n",
    "    prec = precision_score(y_test, y_pred, average=\"macro\")\n",
    "    rec  = recall_score(y_test, y_pred, average=\"macro\")\n",
    "    f1   = f1_score(y_test, y_pred, average=\"macro\")\n",
    "    \n",
    "    print(f\"  → Accuracy:  {acc:.4f}\")\n",
    "    print(f\"    Precision: {prec:.4f}\")\n",
    "    print(f\"    Recall:    {rec:.4f}\")\n",
    "    print(f\"    F1-score:  {f1:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74bfa5a4-0374-4094-9c17-317eb88f8ca0",
   "metadata": {},
   "source": [
    "### Detailed Regression Model Explanations\n",
    "\n",
    "Below is a concise description of how each regression model works, followed by comments on its performance in both 10-fold cross-validation and an 80/20 train/test split.\n",
    "\n",
    "---\n",
    "\n",
    "#### Lasso Regression  \n",
    "Lasso adds an L1 penalty to the ordinary least-squares objective, which encourages many coefficients to become exactly zero. This effectively performs feature selection by discarding less informative inputs and retaining only the strongest predictors.  \n",
    "- **Behavior here**: Achieves the highest \\(R^2\\) of all models (~0.48 CV, ~0.53 test), and the lowest errors (MSE ~6.5 CV → ~5.9 test).  \n",
    "- **Why it succeeds**: By zeroing out noisy or redundant features, it prevents overfitting in this high-dimensional setting.\n",
    "\n",
    "---\n",
    "\n",
    "#### Ridge Regression  \n",
    "Ridge adds an L2 penalty (squared coefficient sum) to the loss. Rather than eliminating features, it shrinks all coefficients toward zero, reducing variance when features are correlated.  \n",
    "- **Behavior here**: Moderate performance (CV \\(R^2\\) ~0.38, test ~0.45). Errors are higher than Lasso.  \n",
    "- **Why it trails Lasso**: Shrinking but not removing unhelpful features leaves some noise in the model.\n",
    "\n",
    "---\n",
    "\n",
    "#### ElasticNet  \n",
    "ElasticNet combines L1 and L2 penalties, blending the feature-selection effect of Lasso with the coefficient shrinkage of Ridge. The balance is controlled by the `l1_ratio`.  \n",
    "- **Behavior here**: Very similar to Lasso (CV \\(R^2\\) ~0.48, test ~0.53) and slightly better MSE/MAE.  \n",
    "- **Why it matches Lasso**: The chosen mix of L1/L2 leverages both selection and shrinkage, yielding robust estimates.\n",
    "\n",
    "---\n",
    "\n",
    "#### SVR Pipeline  \n",
    "First applies univariate **SelectKBest** to pick the top 50 features by correlation with the target, then fits a **Support Vector Regressor** with an RBF kernel.  \n",
    "- **Selected features (k = 50):**  \n",
    "  - Input13  \n",
    "  - Input18  \n",
    "  - Input20  \n",
    "  - Input25  \n",
    "  - Input34  \n",
    "  - Input35  \n",
    "  - Input57  \n",
    "  - Input83  \n",
    "  - Input131  \n",
    "  - Input136  \n",
    "  - Input143  \n",
    "  - Input153  \n",
    "  - Input156  \n",
    "  - Input167  \n",
    "  - Input173  \n",
    "  - Input179  \n",
    "  - Input184  \n",
    "  - Input193  \n",
    "  - Input194  \n",
    "  - Input195  \n",
    "  - Input204  \n",
    "  - Input205  \n",
    "  - Input213  \n",
    "  - Input217  \n",
    "  - Input222  \n",
    "  - Input223  \n",
    "  - Input241  \n",
    "  - Input250  \n",
    "  - Input258  \n",
    "  - Input270  \n",
    "  - Input285  \n",
    "  - Input286  \n",
    "  - Input292  \n",
    "  - Input294  \n",
    "  - Input295  \n",
    "  - Input298  \n",
    "  - Input301  \n",
    "  - Input302  \n",
    "  - Input342  \n",
    "  - Input345  \n",
    "  - Input348  \n",
    "  - Input359  \n",
    "  - Input362  \n",
    "  - Input368  \n",
    "  - Input376  \n",
    "  - Input377  \n",
    "  - Input385  \n",
    "  - Input388  \n",
    "  - Input389  \n",
    "  - Input390  \n",
    "- **Behavior here**: Lower performance (CV \\(R^2\\) ~0.30, test ~0.34), higher errors (MSE ~8.7 CV → ~8.3 test).  \n",
    "- **Why it underperforms**: Although nonlinear, SVR may still overfit or underfit if irrelevant features remain or the kernel parameters are not perfectly tuned. Even after removing 350 features, the remaining 50 may still include redundant or weak predictors.\n",
    "\n",
    "---\n",
    "\n",
    "#### k-Nearest Neighbors Regressor  \n",
    "A nonparametric method that predicts each point’s output by averaging the targets of its k nearest neighbors in feature space (here, Euclidean distance).  \n",
    "- **Behavior here**: Poorest results (CV \\(R^2\\) ~0.07, test ~0.12), and the largest errors (MSE ~11–11.7).  \n",
    "- **Why it fails**: The high dimensionality and noise make “nearest” neighbors less meaningful, leading to unstable and biased predictions.\n",
    "\n",
    "---\n",
    "\n",
    "## RFECV + RidgeCV Pipeline\n",
    "\n",
    "This pipeline first standardizes all inputs, then uses **RFECV** (recursive feature elimination with 5-fold CV on MSE) to peel away the least informative features (from 400 down to ~14), and finally fits a **Ridge** model whose α is chosen by **RidgeCV** (5-fold CV over `[0.01, 0.1, 1, 10, 100]`).\n",
    "\n",
    "- **Selected features (RFECV eliminated down to ~14):**  \n",
    "  - Input18  \n",
    "  - Input59  \n",
    "  - Input83  \n",
    "  - Input136  \n",
    "  - Input167  \n",
    "  - Input173  \n",
    "  - Input184  \n",
    "  - Input193  \n",
    "  - Input223  \n",
    "  - Input241  \n",
    "  - Input292  \n",
    "  - Input342  \n",
    "  - Input387  \n",
    "  - Input389  \n",
    "\n",
    "### Behavior here\n",
    "\n",
    "- **Cross-validation**  \n",
    "  - \\(R^2 \\approx 0.51\\)  \n",
    "  - MSE \\(\\approx 6.14\\)  \n",
    "  - MAE \\(\\approx 2.00\\)\n",
    "\n",
    "- **Hold-out (80/20 split)**  \n",
    "  - \\(R^2 \\approx 0.53\\)  \n",
    "  - MSE \\(\\approx 5.68\\)  \n",
    "  - MAE \\(\\approx 1.93\\)\n",
    "\n",
    "### Why it excels\n",
    "\n",
    "1. **Dimensionality reduction**  \n",
    "   RFECV strips away noisy and redundant inputs so that the model sees only the most predictive ~14 features.  \n",
    "\n",
    "2. **Optimized shrinkage**  \n",
    "   RidgeCV finds the “just right” L2 penalty, taming multicollinearity without throwing away too much signal.  \n",
    "\n",
    "3. **Synergy**  \n",
    "   Combining targeted feature selection with tuned regularization yields a better bias–variance trade-off than either technique alone.\n",
    "\n",
    "---\n",
    "\n",
    "**Overall Takeaways**  \n",
    "- The **RFECV + RidgeCV** pipeline now leads the pack in both CV and hold-out tests, surpassing even Lasso/ElasticNet by aggressively reducing noise before regularizing.  \n",
    "- Sparse methods (Lasso/ElasticNet) remain strong baselines, but targeted feature elimination + tuned Ridge delivers the best generalization here.  \n",
    "- Nonlinear approaches (SVR, k-NN) still lag unless paired with more sophisticated dimensionality reduction.  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "b62c53d8-3588-4435-9f4e-8a6e530df68c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== 10-fold CV – average metrics for each model ===\n",
      "                                 R2        MSE       MAE\n",
      "Model                                                   \n",
      "Lasso (alpha=0.114)        0.481601   6.490828  2.031663\n",
      "Ridge (alpha=0.11)         0.375667   7.765346  2.217250\n",
      "ElasticNet (l1_ratio=0.6)  0.478267   6.526011  2.036991\n",
      "SVR Pipeline               0.302444   8.744197  2.372365\n",
      "k-NN Regressor (k=10)      0.072118  11.736047  2.719089\n",
      "RFECV + RidgeCV            0.505155   6.175246  1.998493\n",
      "  → R2:  0.5316\n",
      "    MSE: 5.9301\n",
      "    MAE: 1.9694\n",
      "  → R2:  0.4464\n",
      "    MSE: 7.0078\n",
      "    MAE: 2.0855\n",
      "  → R2:  0.5343\n",
      "    MSE: 5.8959\n",
      "    MAE: 1.9614\n",
      "\n",
      "Model: SVR Pipeline\n",
      "Selected features (SelectKBest, k=50):\n",
      "  - Input13\n",
      "  - Input18\n",
      "  - Input20\n",
      "  - Input25\n",
      "  - Input34\n",
      "  - Input35\n",
      "  - Input57\n",
      "  - Input83\n",
      "  - Input131\n",
      "  - Input136\n",
      "  - Input143\n",
      "  - Input153\n",
      "  - Input156\n",
      "  - Input167\n",
      "  - Input173\n",
      "  - Input179\n",
      "  - Input184\n",
      "  - Input193\n",
      "  - Input194\n",
      "  - Input195\n",
      "  - Input204\n",
      "  - Input205\n",
      "  - Input213\n",
      "  - Input217\n",
      "  - Input222\n",
      "  - Input223\n",
      "  - Input241\n",
      "  - Input250\n",
      "  - Input258\n",
      "  - Input270\n",
      "  - Input285\n",
      "  - Input286\n",
      "  - Input292\n",
      "  - Input294\n",
      "  - Input295\n",
      "  - Input298\n",
      "  - Input301\n",
      "  - Input302\n",
      "  - Input342\n",
      "  - Input345\n",
      "  - Input348\n",
      "  - Input359\n",
      "  - Input362\n",
      "  - Input368\n",
      "  - Input376\n",
      "  - Input377\n",
      "  - Input385\n",
      "  - Input388\n",
      "  - Input389\n",
      "  - Input390\n",
      "  → R2:  0.3435\n",
      "    MSE: 8.3107\n",
      "    MAE: 2.3391\n",
      "  → R2:  0.1222\n",
      "    MSE: 11.1124\n",
      "    MAE: 2.6763\n",
      "\n",
      "Model: RFECV + RidgeCV\n",
      "Selected features (RFECV):\n",
      "  - Input18\n",
      "  - Input59\n",
      "  - Input83\n",
      "  - Input136\n",
      "  - Input167\n",
      "  - Input173\n",
      "  - Input184\n",
      "  - Input193\n",
      "  - Input223\n",
      "  - Input241\n",
      "  - Input292\n",
      "  - Input342\n",
      "  - Input387\n",
      "  - Input389\n",
      "  → R2:  0.5516\n",
      "    MSE: 5.6767\n",
      "    MAE: 1.9328\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.pipeline           import Pipeline\n",
    "from sklearn.preprocessing     import StandardScaler\n",
    "from sklearn.linear_model      import Lasso, Ridge, ElasticNet, RidgeCV\n",
    "from sklearn.feature_selection  import RFECV, SelectKBest, f_regression\n",
    "from sklearn.svm               import SVR\n",
    "from sklearn.neighbors         import KNeighborsRegressor\n",
    "from sklearn.model_selection   import KFold, train_test_split\n",
    "from sklearn.metrics           import r2_score, mean_squared_error, mean_absolute_error\n",
    "\n",
    "# 0) Load data\n",
    "df = pd.read_csv(file_name, sep=\";\")\n",
    "input_cols = [c for c in df.columns if c.lower().startswith(\"input\")]\n",
    "X = df[input_cols]\n",
    "y = df['Output']\n",
    "\n",
    "models = {\n",
    "    \"Lasso (alpha=0.114)\": Pipeline([\n",
    "        (\"scaler\", StandardScaler()),\n",
    "        (\"lasso\", Lasso(alpha=0.114, max_iter=10000, random_state=42))\n",
    "    ]),\n",
    "    \"Ridge (alpha=0.11)\": Pipeline([\n",
    "        (\"scaler\", StandardScaler()),\n",
    "        (\"ridge\", Ridge(alpha=0.11, random_state=42))\n",
    "    ]),\n",
    "    \"ElasticNet (l1_ratio=0.6)\": Pipeline([\n",
    "        (\"scaler\", StandardScaler()),\n",
    "        (\"en\", ElasticNet(alpha=0.114, l1_ratio=0.6, max_iter=5000, random_state=42))\n",
    "    ]),\n",
    "    \"SVR Pipeline\": Pipeline([\n",
    "        (\"select\", SelectKBest(score_func=f_regression, k=50)),\n",
    "        (\"svr\", SVR(kernel=\"rbf\", C=100.0, epsilon=0.1, gamma=\"scale\"))\n",
    "    ]),\n",
    "    \"k-NN Regressor (k=10)\": KNeighborsRegressor(\n",
    "        n_neighbors=10, metric=\"minkowski\"\n",
    "    ),\n",
    "    \"RFECV + RidgeCV\": Pipeline([\n",
    "        (\"scaler\", StandardScaler()),\n",
    "        (\"rfecv\", RFECV(\n",
    "            estimator=Ridge(random_state=42),\n",
    "            step=1,\n",
    "            cv=5,\n",
    "            scoring=\"r2\",\n",
    "            n_jobs=-1\n",
    "        )),\n",
    "        (\"ridgecv\", RidgeCV(\n",
    "            alphas=[0.01, 0.1, 1, 10, 100],\n",
    "            cv=5,\n",
    "            scoring=\"r2\"\n",
    "        ))\n",
    "    ]),\n",
    "}\n",
    "\n",
    "# 1) 10-fold CV – average metrics (as before)\n",
    "cv = KFold(n_splits=10, shuffle=True, random_state=42)\n",
    "scoring = {\n",
    "    'R2': 'r2',\n",
    "    'MSE': 'neg_mean_squared_error',\n",
    "    'MAE': 'neg_mean_absolute_error'\n",
    "}\n",
    "\n",
    "results_cv = []\n",
    "for name, model in models.items():\n",
    "    cv_res = cross_validate(\n",
    "        model, X, y, cv=cv, scoring=scoring,\n",
    "        n_jobs=-1, return_train_score=False\n",
    "    )\n",
    "    results_cv.append({\n",
    "        \"Model\": name,\n",
    "        \"R2\": np.mean(cv_res['test_R2']),\n",
    "        \"MSE\": -np.mean(cv_res['test_MSE']),\n",
    "        \"MAE\": -np.mean(cv_res['test_MAE'])\n",
    "    })\n",
    "\n",
    "metrics_cv_df = pd.DataFrame(results_cv).set_index(\"Model\")\n",
    "print(\"=== 10-fold CV – average metrics for each model ===\")\n",
    "print(metrics_cv_df)\n",
    "\n",
    "# 2) 80/20 train-test split evaluation + print selected features (if any)\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "for name, model in models.items():\n",
    "    # Fit model on training data\n",
    "    model.fit(X_train, y_train)\n",
    "    \n",
    "    # If the model has a SelectKBest step, print which features were selected\n",
    "    if hasattr(model, \"named_steps\") and \"select\" in model.named_steps:\n",
    "        selector = model.named_steps[\"select\"]\n",
    "        mask = selector.get_support()\n",
    "        selected_feats = [feat for feat, use in zip(input_cols, mask) if use]\n",
    "        \n",
    "        print(f\"\\nModel: {name}\")\n",
    "        print(\"Selected features (SelectKBest, k=50):\")\n",
    "        for feat in selected_feats:\n",
    "            print(\"  -\", feat)\n",
    "    \n",
    "    # If the model has an RFECV step, print which features were selected\n",
    "    if hasattr(model, \"named_steps\") and \"rfecv\" in model.named_steps:\n",
    "        rfecv = model.named_steps[\"rfecv\"]\n",
    "        mask = rfecv.get_support()\n",
    "        selected_feats = [feat for feat, use in zip(input_cols, mask) if use]\n",
    "        \n",
    "        print(f\"\\nModel: {name}\")\n",
    "        print(\"Selected features (RFECV):\")\n",
    "        for feat in selected_feats:\n",
    "            print(\"  -\", feat)\n",
    "    \n",
    "    # Compute and print test metrics\n",
    "    y_pred = model.predict(X_test)\n",
    "    r2  = r2_score(y_test, y_pred)\n",
    "    mse = mean_squared_error(y_test, y_pred)\n",
    "    mae = mean_absolute_error(y_test, y_pred)\n",
    "    \n",
    "    print(f\"  → R2:  {r2:.4f}\")\n",
    "    print(f\"    MSE: {mse:.4f}\")\n",
    "    print(f\"    MAE: {mae:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb5554e4-183d-4a38-b51b-97cdc52cfeec",
   "metadata": {},
   "source": [
    "## Principal Component Analysis (PCA) Summary\n",
    "\n",
    "1. **Original Data Shape**  \n",
    "   - We start with a feature matrix **X** of shape **(2000, 400)**, meaning there are 2,000 samples and 400 raw input variables.\n",
    "\n",
    "2. **Explained Variance by All Components**  \n",
    "   - We fit PCA to **all 400 components** and compute the cumulative sum of explained variance ratios.  \n",
    "   - This tells us, for each number of principal components, how much of the total variance in the original data is captured.\n",
    "\n",
    "3. **Selecting Enough Components for 90% Variance**  \n",
    "   - We set a threshold of **90%** cumulative explained variance.  \n",
    "   - Using `np.searchsorted`, we find that **293** principal components are required to reach at least 90% of the total variance.\n",
    "\n",
    "4. **Dimensionality Reduction**  \n",
    "   - We then re-fit PCA with `n_components=293` and transform the original data.  \n",
    "   - The resulting matrix **X_reduced** has shape **(2000, 293)**.\n",
    "\n",
    "---\n",
    "\n",
    "### What This Means\n",
    "\n",
    "- **Dimensionality Reduction**: We have reduced the feature space from 400 dimensions down to 293 dimensions without losing more than 10% of the original variance.  \n",
    "- **Trade-off**: By dropping 107 components (400 → 293), we are discarding only the least informative directions in the data, which likely correspond to noise or redundant information.  \n",
    "- **Next Steps**: This lower-dimensional representation can speed up downstream modeling, reduce overfitting, and simplify feature selection, while still preserving the bulk of the signal present in the original inputs.  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "5fcc2cd3-e673-4f4a-a337-1daa1e05d11c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original X shape: (2000, 400)\n",
      "To achieve 90% cumulative variance, you need 293 components.\n",
      "X_reduced shape: (2000, 293)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.decomposition import PCA\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# 2) Select only the input columns:\n",
    "input_cols = [c for c in df.columns if c.lower().startswith(\"input\")]\n",
    "X = df[input_cols].values\n",
    "print(\"Original X shape:\", X.shape)\n",
    "\n",
    "# 3) Fit PCA on all components:\n",
    "pca_full = PCA().fit(X)\n",
    "cumulative = np.cumsum(pca_full.explained_variance_ratio_)\n",
    "\n",
    "# 4) Compute the number of components needed for 90% cumulative variance:\n",
    "threshold = 0.90\n",
    "k = np.searchsorted(cumulative, threshold) + 1\n",
    "print(f\"To achieve {int(threshold*100)}% cumulative variance, you need {k} components.\")\n",
    "\n",
    "# 5) Project onto those k components:\n",
    "pca = PCA(n_components=k)\n",
    "X_reduced = pca.fit_transform(X)\n",
    "print(\"X_reduced shape:\", X_reduced.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14117f79-7aaa-46f1-8f44-a62771ef79f6",
   "metadata": {},
   "source": [
    "### Impact of PCA on Model Performance\n",
    "\n",
    "After reducing the original 400-dimensional feature space down to 293 principal components (capturing 90% of variance), we re-ran our top classification and regression pipelines. The results show a **significant drop in performance** compared to using the full feature set:\n",
    "\n",
    "- **Classification (10-fold CV)**:  \n",
    "  - SVC Pipeline: 81.6% → 65.9% accuracy  \n",
    "  - k-NN (k=50):    61.3% → 54.4%  \n",
    "  - RF (800, d=30): 68.8% → 57.7%  \n",
    "\n",
    "- **Classification (80/20 split)**:  \n",
    "  - SVC Pipeline: 80.8% → 65.8%  \n",
    "  - k-NN:          61.8% → 54.5%  \n",
    "  - RF:            71.0% → 53.3%  \n",
    "\n",
    "- **Regression (10-fold CV)**:  \n",
    "  - Lasso:    \\(R^2\\) 0.48 → 0.29, MSE ~6.5 → 8.9  \n",
    "  - ElasticNet: 0.48 → 0.31, MSE ~6.5 → 8.7  \n",
    "\n",
    "- **Regression (80/20 split)**:  \n",
    "  - Lasso:    \\(R^2\\) 0.53 → 0.34, MSE ~5.9 → 8.4  \n",
    "  - ElasticNet: 0.53 → 0.35, MSE ~5.9 → 8.2  \n",
    "\n",
    "---\n",
    "\n",
    "#### Why Performance Degrades with PCA\n",
    "\n",
    "1. **Unsupervised reduction**  \n",
    "   PCA captures directions of greatest variance **without regard to the target**. Variance in the data does not always align with the features most predictive of class labels or target values.  \n",
    "\n",
    "2. **Loss of discriminative information**  \n",
    "   By retaining components that explain overall variance, PCA may discard lower-variance dimensions that nonetheless carry critical signals for separation or regression.  \n",
    "\n",
    "3. **Curse of dimensionality mitigation vs. task relevance**  \n",
    "   While reducing dimension can help combat overfitting, here the retained 293 components still include many nuisance directions, and the reduction process itself removes subtle but important patterns.  \n",
    "\n",
    "4. **Model sensitivity**  \n",
    "   Classifiers like SVC and regressors like Lasso rely on specific feature relationships (e.g. marginal correlations) that PCA rotations obscure.\n",
    "\n",
    "---\n",
    "\n",
    "**Conclusion:**  \n",
    "Although PCA preserves most of the raw variance, it is not optimized for predictive accuracy on our targets—especially when many small-variance directions are actually highly informative. For supervised tasks, techniques such as **supervised feature selection**, **Partial Least Squares (PLS)**, or **embedded methods** (e.g. Lasso feature selection) often outperform vanilla PCA.  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "23751cea-06b0-422d-a210-fe2e47020b12",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== 10-fold CV – average metrics for each model ===\n",
      "                              Accuracy  Precision    Recall  F1-score\n",
      "Model                                                                \n",
      "SVC Pipeline                    0.6595   0.661298  0.658971  0.658101\n",
      "k-NN (Manhattan, k=50)          0.5485   0.549629  0.547184  0.542676\n",
      "RF (800 trees, max_depth=30)    0.5710   0.571929  0.570049  0.567665\n",
      "\n",
      "=== Test set results (80/20 split) ===\n",
      "                              Accuracy  Precision    Recall  F1-score\n",
      "Model                                                                \n",
      "SVC Pipeline                    0.6425   0.642814  0.641932  0.641692\n",
      "k-NN (Manhattan, k=50)          0.5600   0.560542  0.558701  0.555993\n",
      "RF (800 trees, max_depth=30)    0.5550   0.554817  0.554225  0.553392\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.feature_selection import SelectKBest, f_classif\n",
    "from sklearn.svm import SVC, LinearSVC\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.ensemble import (\n",
    "    RandomForestClassifier,\n",
    "    ExtraTreesClassifier,\n",
    "    HistGradientBoostingClassifier\n",
    ")\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import StratifiedKFold, cross_validate, train_test_split\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score,\n",
    "    precision_score,\n",
    "    recall_score,\n",
    "    f1_score\n",
    ")\n",
    "\n",
    "# 0) Load data\n",
    "y = df['Class']\n",
    "\n",
    "# 1) Define models\n",
    "models = {\n",
    "    \"SVC Pipeline\": Pipeline([\n",
    "        (\"select\", SelectKBest(score_func=f_classif, k=5)),\n",
    "        (\"svc\",    SVC(kernel=\"rbf\", C=10.0, gamma=\"scale\", random_state=42))\n",
    "    ]),\n",
    "    \"k-NN (Manhattan, k=50)\": KNeighborsClassifier(\n",
    "        n_neighbors=50, metric=\"manhattan\"\n",
    "    ),\n",
    "    \"RF (800 trees, max_depth=30)\": RandomForestClassifier(\n",
    "        n_estimators=800, max_depth=30, random_state=42\n",
    "    ),\n",
    "}\n",
    "\n",
    "# 2) 10-fold Stratified CV – only average metrics\n",
    "cv = StratifiedKFold(n_splits=10, shuffle=True, random_state=42)\n",
    "scoring = {\n",
    "    'accuracy':  'accuracy',\n",
    "    'precision': 'precision_macro',\n",
    "    'recall':    'recall_macro',\n",
    "    'f1':        'f1_macro'\n",
    "}\n",
    "\n",
    "results_cv = []\n",
    "for name, model in models.items():\n",
    "    cv_res = cross_validate(\n",
    "        model, X_reduced, y, cv=cv, scoring=scoring, n_jobs=-1, return_train_score=False\n",
    "    )\n",
    "    results_cv.append({\n",
    "        \"Model\":     name,\n",
    "        \"Accuracy\":  np.mean(cv_res['test_accuracy']),\n",
    "        \"Precision\": np.mean(cv_res['test_precision']),\n",
    "        \"Recall\":    np.mean(cv_res['test_recall']),\n",
    "        \"F1-score\":  np.mean(cv_res['test_f1'])\n",
    "    })\n",
    "\n",
    "metrics_cv_df = pd.DataFrame(results_cv).set_index(\"Model\")\n",
    "print(\"=== 10-fold CV – average metrics for each model ===\")\n",
    "print(metrics_cv_df)\n",
    "\n",
    "# 3) 80/20 train-test split evaluation\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X_reduced, y, test_size=0.2, random_state=41, stratify=y\n",
    ")\n",
    "\n",
    "results_test = []\n",
    "for name, model in models.items():\n",
    "    model.fit(X_train, y_train)\n",
    "    y_pred = model.predict(X_test)\n",
    "    results_test.append({\n",
    "        \"Model\":     name,\n",
    "        \"Accuracy\":  accuracy_score(y_test, y_pred),\n",
    "        \"Precision\": precision_score(y_test, y_pred, average='macro'),\n",
    "        \"Recall\":    recall_score(y_test, y_pred, average='macro'),\n",
    "        \"F1-score\":  f1_score(y_test, y_pred, average='macro')\n",
    "    })\n",
    "\n",
    "metrics_test_df = pd.DataFrame(results_test).set_index(\"Model\")\n",
    "print(\"\\n=== Test set results (80/20 split) ===\")\n",
    "print(metrics_test_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "03781852-6d56-4480-899d-92ed63655197",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== 10-fold CV – average metrics for each model ===\n",
      "                                 R2       MSE       MAE\n",
      "Model                                                  \n",
      "Lasso (alpha=0.114)        0.304631  8.752247  2.354744\n",
      "ElasticNet (l1_ratio=0.6)  0.325761  8.470025  2.315408\n",
      "\n",
      "=== 80/20 Train-Test Split – metrics for each model ===\n",
      "                                 R2       MSE       MAE\n",
      "Model                                                  \n",
      "Lasso (alpha=0.114)        0.351989  8.203404  2.269200\n",
      "ElasticNet (l1_ratio=0.6)  0.373457  7.931644  2.222601\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.feature_selection import SelectKBest, f_regression\n",
    "from sklearn.linear_model import Lasso, Ridge, ElasticNet\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import KFold, cross_validate, train_test_split\n",
    "from sklearn.metrics import r2_score, mean_squared_error, mean_absolute_error\n",
    "\n",
    "# 0) Load data\n",
    "y = df['Output']\n",
    "\n",
    "# 1) Define models\n",
    "models = {\n",
    "    \"Lasso (alpha=0.114)\": Pipeline([\n",
    "        (\"scaler\", StandardScaler()),\n",
    "        (\"lasso\", Lasso(alpha=0.114, max_iter=10000, random_state=42))\n",
    "    ]),\n",
    "    \"ElasticNet (l1_ratio=0.6)\": Pipeline([\n",
    "        (\"scaler\", StandardScaler()),\n",
    "        (\"en\", ElasticNet(alpha=0.114, l1_ratio=0.6, max_iter=5000, random_state=42))\n",
    "    ]),\n",
    "}\n",
    "\n",
    "# 2) 10-fold CV – average metrics\n",
    "cv = KFold(n_splits=10, shuffle=True, random_state=42)\n",
    "scoring = {\n",
    "    'R2': 'r2',\n",
    "    'MSE': 'neg_mean_squared_error',\n",
    "    'MAE': 'neg_mean_absolute_error'\n",
    "}\n",
    "\n",
    "results_cv = []\n",
    "for name, model in models.items():\n",
    "    cv_res = cross_validate(\n",
    "        model, X_reduced, y, cv=cv, scoring=scoring,\n",
    "        n_jobs=-1, return_train_score=False\n",
    "    )\n",
    "    results_cv.append({\n",
    "        \"Model\": name,\n",
    "        \"R2\": np.mean(cv_res['test_R2']),\n",
    "        \"MSE\": -np.mean(cv_res['test_MSE']),\n",
    "        \"MAE\": -np.mean(cv_res['test_MAE'])\n",
    "    })\n",
    "\n",
    "metrics_cv_df = pd.DataFrame(results_cv).set_index(\"Model\")\n",
    "print(\"=== 10-fold CV – average metrics for each model ===\")\n",
    "print(metrics_cv_df)\n",
    "\n",
    "# 3) 80/20 train-test split evaluation\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X_reduced, y, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "results_tt = []\n",
    "for name, model in models.items():\n",
    "    model.fit(X_train, y_train)\n",
    "    y_pred = model.predict(X_test)\n",
    "    results_tt.append({\n",
    "        \"Model\": name,\n",
    "        \"R2\": r2_score(y_test, y_pred),\n",
    "        \"MSE\": mean_squared_error(y_test, y_pred),\n",
    "        \"MAE\": mean_absolute_error(y_test, y_pred)\n",
    "    })\n",
    "\n",
    "metrics_tt_df = pd.DataFrame(results_tt).set_index(\"Model\")\n",
    "print(\"\\n=== 80/20 Train-Test Split – metrics for each model ===\")\n",
    "print(metrics_tt_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9b3e6e7-75e5-4a32-be22-a4a34f73b8c8",
   "metadata": {},
   "source": [
    "### Neural Network Model Summary\n",
    "\n",
    "1. **Architecture**  \n",
    "   - **Input layer**: 400 features  \n",
    "   - **Hidden layers**:  \n",
    "     - Dense(400 → 128) + ReLU  \n",
    "     - Dense(128 → 64) + ReLU  \n",
    "   - **Output layer**: Dense(64 → 1) for regression  \n",
    "\n",
    "2. **Training Dynamics**  \n",
    "   - Trained for 500 epochs with Adam (LR=1e-3, batch size 32).  \n",
    "   - The **training loss (MSE)** started around 0.15, dipped below 0.02 by epoch 260, and stabilized around 0.02–0.04.  \n",
    "   - Some fluctuation in loss indicates minor overfitting/noise, but the steady downward trend shows the network successfully learned patterns in the data.\n",
    "\n",
    "3. **Final Test Performance**  \n",
    "   - **Test MSE**: 7.53  \n",
    "   - **Test R²**: 0.4055  \n",
    "\n",
    "4. **Assessment**  \n",
    "   - The network improves over a trivial constant predictor (which would have R²≤0), but its R² (~0.41) is still lower than our best linear baselines (Lasso/ElasticNet achieved ~0.53).  \n",
    "   - This suggests that, despite fitting the training data well (low MSE), the model generalizes only moderately. Further regularization, architecture tuning, or more data might be needed to outperform simpler models.  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "702d2d22-17aa-4f25-aa6f-0efe27f4350f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10/500 – Loss: 0.1365\n",
      "Epoch 20/500 – Loss: 0.1918\n",
      "Epoch 30/500 – Loss: 0.1087\n",
      "Epoch 40/500 – Loss: 0.1242\n",
      "Epoch 50/500 – Loss: 0.0980\n",
      "Epoch 60/500 – Loss: 0.0640\n",
      "Epoch 70/500 – Loss: 0.1382\n",
      "Epoch 80/500 – Loss: 0.0558\n",
      "Epoch 90/500 – Loss: 0.0692\n",
      "Epoch 100/500 – Loss: 0.0878\n",
      "Epoch 110/500 – Loss: 0.0619\n",
      "Epoch 120/500 – Loss: 0.0844\n",
      "Epoch 130/500 – Loss: 0.0534\n",
      "Epoch 140/500 – Loss: 0.0527\n",
      "Epoch 150/500 – Loss: 0.0663\n",
      "Epoch 160/500 – Loss: 0.0426\n",
      "Epoch 170/500 – Loss: 0.0374\n",
      "Epoch 180/500 – Loss: 0.0723\n",
      "Epoch 190/500 – Loss: 0.0304\n",
      "Epoch 200/500 – Loss: 0.0441\n",
      "Epoch 210/500 – Loss: 0.0474\n",
      "Epoch 220/500 – Loss: 0.0353\n",
      "Epoch 230/500 – Loss: 0.0397\n",
      "Epoch 240/500 – Loss: 0.0281\n",
      "Epoch 250/500 – Loss: 0.0223\n",
      "Epoch 260/500 – Loss: 0.0376\n",
      "Epoch 270/500 – Loss: 0.0230\n",
      "Epoch 280/500 – Loss: 0.0378\n",
      "Epoch 290/500 – Loss: 0.0241\n",
      "Epoch 300/500 – Loss: 0.0341\n",
      "Epoch 310/500 – Loss: 0.0285\n",
      "Epoch 320/500 – Loss: 0.0203\n",
      "Epoch 330/500 – Loss: 0.0347\n",
      "Epoch 340/500 – Loss: 0.0161\n",
      "Epoch 350/500 – Loss: 0.0286\n",
      "Epoch 360/500 – Loss: 0.0224\n",
      "Epoch 370/500 – Loss: 0.0105\n",
      "Epoch 380/500 – Loss: 0.0383\n",
      "Epoch 390/500 – Loss: 0.0120\n",
      "Epoch 400/500 – Loss: 0.0163\n",
      "Epoch 410/500 – Loss: 0.0276\n",
      "Epoch 420/500 – Loss: 0.0105\n",
      "Epoch 430/500 – Loss: 0.0186\n",
      "Epoch 440/500 – Loss: 0.0175\n",
      "Epoch 450/500 – Loss: 0.0163\n",
      "Epoch 460/500 – Loss: 0.0141\n",
      "Epoch 470/500 – Loss: 0.0113\n",
      "Epoch 480/500 – Loss: 0.0275\n",
      "Epoch 490/500 – Loss: 0.0154\n",
      "Epoch 500/500 – Loss: 0.0228\n",
      "\n",
      "Test MSE: 7.6121\n",
      "Test R²: 0.3987\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import r2_score, mean_squared_error\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "# 0) Hyperparameters\n",
    "BATCH_SIZE = 32\n",
    "EPOCHS = 500\n",
    "LEARNING_RATE = 1e-3\n",
    "\n",
    "# 1) Load data\n",
    "df = pd.read_csv(file_name, sep=\";\")\n",
    "input_cols = [c for c in df.columns if c.lower().startswith(\"input\")]\n",
    "X = df[input_cols].values.astype(np.float32)\n",
    "y = df['Output'].values.reshape(-1, 1).astype(np.float32)\n",
    "\n",
    "# 2) Split into train/test sets and standardize features\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42\n",
    ")\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "# 3) Prepare DataLoaders\n",
    "train_ds = TensorDataset(torch.from_numpy(X_train), torch.from_numpy(y_train))\n",
    "train_loader = DataLoader(train_ds, batch_size=BATCH_SIZE, shuffle=True)\n",
    "X_test_t = torch.from_numpy(X_test)\n",
    "y_test_t = torch.from_numpy(y_test)\n",
    "\n",
    "# 4) Define the neural network model\n",
    "class RegressionNN(nn.Module):\n",
    "    def __init__(self, input_dim):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(input_dim, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, 1)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "# 5) Initialize model, loss function, and optimizer\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = RegressionNN(X_train.shape[1]).to(device)\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
    "\n",
    "# 6) Training loop\n",
    "for epoch in range(1, EPOCHS + 1):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    for xb, yb in train_loader:\n",
    "        xb, yb = xb.to(device), yb.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        preds = model(xb)\n",
    "        loss = criterion(preds, yb)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item() * xb.size(0)\n",
    "    epoch_loss = running_loss / len(train_loader.dataset)\n",
    "    if epoch % 10 == 0:\n",
    "        print(f\"Epoch {epoch}/{EPOCHS} – Loss: {epoch_loss:.4f}\")\n",
    "\n",
    "# 7) Evaluation on the test set\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    preds_test = model(X_test_t.to(device)).cpu().numpy()\n",
    "\n",
    "mse = mean_squared_error(y_test, preds_test)\n",
    "r2 = r2_score(y_test, preds_test)\n",
    "print(f\"\\nTest MSE: {mse:.4f}\")\n",
    "print(f\"Test R²: {r2:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "fd980275-7fca-48ad-b3b9-c4482e84a832",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error: The file path 'validation_data.csv' is invalid or the file does not exist.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.pipeline           import Pipeline\n",
    "from sklearn.preprocessing     import StandardScaler\n",
    "from sklearn.feature_selection  import SelectKBest, f_classif, RFECV\n",
    "from sklearn.svm               import SVC\n",
    "from sklearn.linear_model      import Ridge, RidgeCV\n",
    "from sklearn.model_selection   import train_test_split\n",
    "from sklearn.metrics           import (\n",
    "    r2_score, mean_squared_error, mean_absolute_error,\n",
    "    accuracy_score, precision_score, recall_score, f1_score\n",
    ")\n",
    "\n",
    "def evaluate(file_path):\n",
    "    \"\"\"\n",
    "    Reads a CSV from file_path (semicolon-delimited) and assumes:\n",
    "      - columns starting with \"Input\" are features\n",
    "      - column 'Output' is the regression target\n",
    "      - column 'Class' is the classification target\n",
    "\n",
    "    If file_path does not exist or cannot be read, prints an error message.\n",
    "\n",
    "    For regression: builds the \"RFECV + RidgeCV\" pipeline, trains on an 80/20 split,\n",
    "      and prints:\n",
    "        - the features selected by RFECV\n",
    "        - R2, MSE, MAE metrics on the test set\n",
    "\n",
    "    For classification: builds the \"SelectKBest + SVC\" pipeline, trains on an 80/20 split\n",
    "      (with stratify=y), and prints:\n",
    "        - the features selected by SelectKBest\n",
    "        - accuracy, precision, recall, F1 metrics on the test set\n",
    "    \"\"\"\n",
    "    # Check if the file exists before attempting to load\n",
    "    if not os.path.isfile(file_path):\n",
    "        print(f\"Error: The file path '{file_path}' is invalid or the file does not exist.\")\n",
    "        return\n",
    "\n",
    "    try:\n",
    "        # 1) Load data\n",
    "        df = pd.read_csv(file_path, sep=\";\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error reading '{file_path}': {e}\")\n",
    "        return\n",
    "\n",
    "    input_cols = [c for c in df.columns if c.lower().startswith(\"input\")]\n",
    "    X = df[input_cols]\n",
    "    y_reg = df['Output']\n",
    "    y_clf = df['Class']\n",
    "    \n",
    "    # 2) Split into training/test sets\n",
    "    #    - regression (no stratify)\n",
    "    X_train_reg, X_test_reg, y_train_reg, y_test_reg = train_test_split(\n",
    "        X, y_reg, test_size=0.2, random_state=42\n",
    "    )\n",
    "    #    - classification (stratify=y_clf)\n",
    "    X_train_clf, X_test_clf, y_train_clf, y_test_clf = train_test_split(\n",
    "        X, y_clf, test_size=0.2, random_state=42, stratify=y_clf\n",
    "    )\n",
    "    \n",
    "    # 3) Regression pipeline: RFECV + RidgeCV\n",
    "    rfecv_ridge = Pipeline([\n",
    "        (\"scaler\", StandardScaler()),\n",
    "        (\"rfecv\", RFECV(\n",
    "            estimator=Ridge(random_state=42),\n",
    "            step=1,\n",
    "            cv=5,\n",
    "            scoring=\"r2\",\n",
    "            n_jobs=-1\n",
    "        )),\n",
    "        (\"ridgecv\", RidgeCV(\n",
    "            alphas=[0.01, 0.1, 1, 10, 100],\n",
    "            cv=5,\n",
    "            scoring=\"r2\"\n",
    "        ))\n",
    "    ])\n",
    "    # Fit regression model\n",
    "    rfecv_ridge.fit(X_train_reg, y_train_reg)\n",
    "    \n",
    "    # 3a) Extract features selected by RFECV\n",
    "    rfecv_selector = rfecv_ridge.named_steps[\"rfecv\"]\n",
    "    mask_reg = rfecv_selector.get_support()\n",
    "    selected_reg_feats = [feat for feat, use in zip(input_cols, mask_reg) if use]\n",
    "    \n",
    "    # 3b) Regression predictions and metrics on test set\n",
    "    y_pred_reg = rfecv_ridge.predict(X_test_reg)\n",
    "    r2 = r2_score(y_test_reg, y_pred_reg)\n",
    "    mse = mean_squared_error(y_test_reg, y_pred_reg)\n",
    "    mae = mean_absolute_error(y_test_reg, y_pred_reg)\n",
    "    \n",
    "    # 4) Classification pipeline: SelectKBest + SVC\n",
    "    svc_pipeline = Pipeline([\n",
    "        (\"select\", SelectKBest(score_func=f_classif, k=10)),\n",
    "        (\"svc\",    SVC(kernel=\"rbf\", C=100.0, gamma=\"scale\", random_state=42))\n",
    "    ])\n",
    "    # Fit classification model\n",
    "    svc_pipeline.fit(X_train_clf, y_train_clf)\n",
    "    \n",
    "    # 4a) Extract features selected by SelectKBest\n",
    "    svc_selector = svc_pipeline.named_steps[\"select\"]\n",
    "    mask_clf = svc_selector.get_support()\n",
    "    selected_clf_feats = [feat for feat, use in zip(input_cols, mask_clf) if use]\n",
    "    \n",
    "    # 4b) Classification predictions and metrics on test set\n",
    "    y_pred_clf = svc_pipeline.predict(X_test_clf)\n",
    "    acc  = accuracy_score(y_test_clf, y_pred_clf)\n",
    "    prec = precision_score(y_test_clf, y_pred_clf, average=\"macro\")\n",
    "    rec  = recall_score(y_test_clf, y_pred_clf, average=\"macro\")\n",
    "    f1   = f1_score(y_test_clf, y_pred_clf, average=\"macro\")\n",
    "    \n",
    "    # 5) Print results\n",
    "    print(\"=== RFECV + RidgeCV (Regression) ===\")\n",
    "    print(\"Selected features (RFECV):\")\n",
    "    for feat in selected_reg_feats:\n",
    "        print(\"  -\", feat)\n",
    "    print(f\"\\nRegression Metrics on Test Set:\")\n",
    "    print(f\"  R2 : {r2:.4f}\")\n",
    "    print(f\"  MSE: {mse:.4f}\")\n",
    "    print(f\"  MAE: {mae:.4f}\\n\")\n",
    "    \n",
    "    print(\"=== SelectKBest + SVC (Classification) ===\")\n",
    "    print(\"Selected features (SelectKBest, k=10):\")\n",
    "    for feat in selected_clf_feats:\n",
    "        print(\"  -\", feat)\n",
    "    print(f\"\\nClassification Metrics on Test Set:\")\n",
    "    print(f\"  Accuracy : {acc:.4f}\")\n",
    "    print(f\"  Precision: {prec:.4f}\")\n",
    "    print(f\"  Recall   : {rec:.4f}\")\n",
    "    print(f\"  F1-score : {f1:.4f}\")\n",
    "\n",
    "# Example usage:\n",
    "evaluate(\"validation_data.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccc135f7-d9d9-4428-88f7-6e19640c6f48",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
